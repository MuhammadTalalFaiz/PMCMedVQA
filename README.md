# PMCMedVQA
In this project, we developed a MedVQA system designed to answer medical questions based on visual and textual data. We used the PMC-VQA dataset, which includes medical images and corresponding questions and answers. Our system integrates two advanced models: CLIP, which processes images, and BERT, which processes text. By combining these models, we aimed to create a system that can understand and respond to medical queries effectively.
Our approach involved preprocessing the dataset, training the combined model, and evaluating its performance. We found that our model could accurately answer a variety of medical questions, demonstrating the potential of multimodal deep learning for medical applications. The results are promising, suggesting that this method could be useful in real-world medical settings, providing support to healthcare professionals by answering complex visual questions. Future work will focus on improving the model's accuracy and robustness by using larger datasets and optimizing the training process.
